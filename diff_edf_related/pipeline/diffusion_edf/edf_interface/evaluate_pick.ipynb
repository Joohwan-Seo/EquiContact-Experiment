{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from typing import Tuple, Dict, Optional, Union, Any\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from edf.pc_utils import draw_geometry, create_o3d_points, get_plotly_fig\n",
    "from edf.data import PointCloud, SE3, TargetPoseDemo, DemoSequence, DemoSeqDataset, gzip_load\n",
    "from edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from edf.agent import PickAgent, PlaceAgent\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision= 3, sci_mode=False, linewidth=120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define utility functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_pointcloud(**kwargs) -> Tuple[PointCloud, PointCloud]:\n",
    "\n",
    "    ################### Write your custom codes here ###################\n",
    "    dir, idx, pick_or_place = kwargs['dir'], kwargs['idx'], kwargs['pick_or_place']\n",
    "\n",
    "    demos = DemoSeqDataset(dataset_dir=dir, annotation_file=\"data.yaml\")\n",
    "    demo: DemoSequence = demos[idx]\n",
    "    if pick_or_place == 'pick':\n",
    "        demo: TargetPoseDemo = demo[0]\n",
    "    elif pick_or_place == 'place':\n",
    "        demo: TargetPoseDemo = demo[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong value for pick_or_place argument: {pick_or_place}\")\n",
    "\n",
    "    scene_pcd: PointCloud = demo.scene_pc\n",
    "    grasp_pcd: PointCloud = demo.grasp_pc\n",
    "    target_pose: SE3 = demo.target_poses\n",
    "    ####################################################################\n",
    "\n",
    "    return scene_pcd, grasp_pcd\n",
    "\n",
    "\n",
    "def visualize(scene_pcd: PointCloud, grasp_pcd: PointCloud, poses: SE3, query: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, show_sample_points: bool = False):\n",
    "    \n",
    "    grasp_pl = grasp_pcd.plotly(point_size=1.0, name=\"grasp\")\n",
    "    grasp_geometry = [grasp_pl]\n",
    "    if query is not None:\n",
    "        query_points, query_attention = query\n",
    "        query_opacity = query_attention ** 1\n",
    "        query_pl = PointCloud.points_to_plotly(pcd=query_points, point_size=15.0, opacity=query_opacity / query_opacity.max())#, custom_data={'attention': query_attention.cpu()})\n",
    "        grasp_geometry.append(query_pl)\n",
    "    fig_grasp = get_plotly_fig(\"Grasp\")\n",
    "    fig_grasp = fig_grasp.add_traces(grasp_geometry)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    scene_pl = scene_pcd.plotly(point_size=1.0, name='scene')\n",
    "    placement_geometry = [scene_pl]\n",
    "    transformed_grasp_pcd = grasp_pcd.transformed(poses)\n",
    "    for i in range(len(poses)):\n",
    "        pose_pl = transformed_grasp_pcd[i].plotly(point_size=1.0, name=f'pose_{i}')\n",
    "        placement_geometry.append(pose_pl)\n",
    "    if show_sample_points:\n",
    "        sample_pl = PointCloud.points_to_plotly(pcd=poses.points, point_size=7.0, colors=[0.2, 0.5, 0.8], name=f'sample_points')\n",
    "        placement_geometry.append(sample_pl)\n",
    "    fig_sample = get_plotly_fig(\"Sampled Placement\")\n",
    "    fig_sample = fig_sample.add_traces(placement_geometry)\n",
    "    \n",
    "    trace_dict = {}\n",
    "    visiblility_list = []\n",
    "    for i, trace in enumerate(fig_sample.data):\n",
    "        trace_dict[trace.name] = i\n",
    "        if trace.name[:4] == 'pose':\n",
    "            trace.visible = False\n",
    "            visiblility_list.append(False)\n",
    "        else:\n",
    "            visiblility_list.append(trace.visible)\n",
    "\n",
    "    # Define sliders\n",
    "    steps = []\n",
    "    for i in range(len(poses)):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": visiblility_list.copy()},\n",
    "                {\"title\": \"Visualizing pose_\" + str(i)}],  # layout attribute\n",
    "        )\n",
    "        step[\"args\"][0][\"visible\"][trace_dict[f'pose_{i}']] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Pose: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig_sample.update_layout(\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    fig_sample.data[trace_dict[f'pose_0']].visible = True\n",
    "\n",
    "\n",
    "\n",
    "    return fig_grasp, fig_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and warm-up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# device = 'cpu'\n",
    "unit_len = 0.01\n",
    "##### Initialize Pick Agent #####\n",
    "pick_agent_config_dir = \"config/agent_config/pick_agent.yaml\"\n",
    "pick_agent_param_dir = \"checkpoint/mug_10_demo/pick/model_iter_600.pt\"\n",
    "max_N_query_pick = 1\n",
    "langevin_dt_pick = 0.001\n",
    "\n",
    "pick_agent = PickAgent(config_dir=pick_agent_config_dir, \n",
    "                       device = device,\n",
    "                       max_N_query = max_N_query_pick, \n",
    "                       langevin_dt = langevin_dt_pick).requires_grad_(False)\n",
    "\n",
    "pick_agent.load(pick_agent_param_dir)\n",
    "pick_agent.warmup(warmup_iters=10, N_poses=100, N_points_scene=2000)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([Rescale(rescale_factor=1/unit_len),\n",
    "                         Downsample(voxel_size=1.7, coord_reduction=\"average\"),\n",
    "                         NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))])\n",
    "grasp_proc_fn = Compose([\n",
    "                         Rescale(rescale_factor=1/unit_len),\n",
    "                         Downsample(voxel_size=1.4, coord_reduction=\"average\"),\n",
    "                         NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))])\n",
    "recover_scale = Rescale(rescale_factor=unit_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 0\n",
    "#scene_raw, grasp_raw = get_raw_pointcloud(dir='demo/test_demo', idx=file_idx, pick_or_place='pick')           # Trained (seen) demo\n",
    "scene_raw, grasp_raw = get_raw_pointcloud(dir='demo/test_unseen_demo', idx=file_idx, pick_or_place='pick')     # Out-of-distribution (unseen) demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference: Sample grasp poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "\n",
    "##### Sample Pick Poses #####\n",
    "T_seed = 100\n",
    "pick_policy = 'sorted'\n",
    "pick_mh_iter = 1000\n",
    "pick_langevin_iter = 300\n",
    "pick_dist_temp = 1.\n",
    "pick_policy_temp = 1.\n",
    "pick_optim_iter = 100\n",
    "pick_optim_lr = 0.005\n",
    "\n",
    "Ts, edf_outputs, logs = pick_agent.forward(scene=scene_proc, T_seed=T_seed, policy = pick_policy, mh_iter=pick_mh_iter, langevin_iter=pick_langevin_iter, \n",
    "                                            temperature=pick_dist_temp, policy_temperature=pick_policy_temp, optim_iter=pick_optim_iter, optim_lr=pick_optim_lr)\n",
    "\n",
    "pick_poses = recover_scale(SE3(Ts.cpu()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_poses = 30\n",
    "fig_grasp, fig_sample = visualize(scene_pcd=scene_raw, grasp_pcd=grasp_raw, poses=pick_poses[:n_best_poses], query=(edf_outputs['query_points'] * unit_len, edf_outputs['query_attention']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_grasp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edf_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ca6f102110478a902c72711f56796277cbbc7426fc6ac3f33fb30e1302870e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
