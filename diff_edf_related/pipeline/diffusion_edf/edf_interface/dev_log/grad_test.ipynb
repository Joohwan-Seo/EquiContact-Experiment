{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "\n",
    "from edf_interface import data\n",
    "from diffusion_edf.gnn_data import FeaturedPoints\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf.trainer import DiffusionEdfTrainer\n",
    "from diffusion_edf.visualize import visualize_pose\n",
    "from diffusion_edf.agent import DiffusionEdfAgent\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_cluster, torch_scatter\n",
    "from edf_interface.utils.collision_utils import _pcd_energy, check_pcd_collision, _se3_adjoint_lie_grad\n",
    "from edf_interface.data.pcd_utils import transform_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "task_type = 'place'\n",
    "config_root_dir = 'configs/sapien'\n",
    "testset = data.DemoDataset(dataset_dir='demo/sapien_demo_20230625')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "demo = testset[idx][0 if task_type == 'pick' else 1]\n",
    "scene_pcd, grasp_pcd, target_poses = demo.scene_pcd, demo.grasp_pcd, demo.target_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scene_pcd.points\n",
    "# y = torch.stack([pcd.points for pcd in grasp_pcd.transformed(target_poses)], dim=0)\n",
    "y = grasp_pcd.points.unsqueeze(0)\n",
    "Ts = target_poses.poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_r = 0.03\n",
    "\n",
    "energy, grad = _pcd_energy(x, transform_points(y, Ts, batched_pcd=True), cutoff_r=cutoff_r, eps = 0.001, max_num_neighbor=100, cluster_method='knn')\n",
    "adj_grad = _se3_adjoint_lie_grad(target_poses.poses, grad)\n",
    "energy, grad, adj_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "dt = 0.001\n",
    "lie = torch.eye(6) * dt\n",
    "lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(6):\n",
    "    new_Ts = data.se3._multiply(data.se3._exp_map(lie[idx].unsqueeze(0)), Ts)\n",
    "    y_new = transform_points(points=y, Ts=new_Ts, batched_pcd=True)\n",
    "    energy_new, grad_new = _pcd_energy(x, y_new, cutoff_r=cutoff_r, eps = 0.001, max_num_neighbor=100, cluster_method='knn')\n",
    "    num_grad = (energy_new - energy) / dt\n",
    "    print(f\"analytic_grad: {grad[0,idx].item()} || num_grad: {num_grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(6):\n",
    "    new_Ts = data.se3._multiply(Ts, data.se3._exp_map(lie[idx].unsqueeze(0)))\n",
    "    y_new = transform_points(points=y, Ts=new_Ts, batched_pcd=True)\n",
    "    energy_new, grad_new = _pcd_energy(x, y_new, cutoff_r=cutoff_r, eps = 0.001, max_num_neighbor=100, cluster_method='knn')\n",
    "    num_grad = (energy_new - energy) / dt\n",
    "    print(f\"analytic_grad: {adj_grad[0,idx].item()} || num_grad: {num_grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edf_interface.data import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def _se3_adjoint_lie_grad(Ts: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Ts (torch.Tensor): (..., 7), (qw, qx, qy, qz, x, y, z)\n",
    "        grad (torch.Tensor): (..., 6), (rx, ry, rz, vx, vy, vz)\n",
    "\n",
    "    Returns:\n",
    "        adjoint_grad (torch.Tensor): (..., 6), (rx, ry, rz, vx, vy, vz)\n",
    "\n",
    "    Note:\n",
    "    L_v f(g_0 g x) = L_{[Ad_g0]v} f(g g_0 x)\n",
    "    => Grad_{g} f(g_0 g x) = Grad_{g} [Ad_g0]^{Transpose} f(g g_0 x)\n",
    "    Note that gradient takes the transpose of adjoint matrix!!\n",
    "    [Ad_T]^{Transpose} = [\n",
    "        [R^{-1},   -R^{-1} skew(p)],\n",
    "        [     0,        R^{-1}    ]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    assert Ts.shape[-1] == 7, f\"{Ts.shape}\"\n",
    "    assert grad.shape[-1] == 6, f\"{grad.shape}\"\n",
    "    assert Ts.shape[:1] == grad.shape[:1], f\"{Ts.shape}, {grad.shape}\"\n",
    "\n",
    "    qinv = transforms.quaternion_invert(Ts[..., :4]) # (..., 4)\n",
    "    adj_grad_R = grad[..., :3] - torch.cross(Ts[..., 4:], grad[..., 3:]) # (..., 3)\n",
    "    adj_grad_R = transforms.quaternion_apply(qinv, adj_grad_R) # (..., 3)\n",
    "    adj_grad_v = transforms.quaternion_apply(qinv, grad[..., 3:]) # (..., 3)\n",
    "    \n",
    "    adj_grad = torch.cat([adj_grad_R, adj_grad_v], dim=-1) # (..., 6)\n",
    "\n",
    "    return adj_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "def _optimize_pcd_collision_once(x: torch.Tensor, \n",
    "                                 y: torch.Tensor, \n",
    "                                 Ts: torch.Tensor,\n",
    "                                 dt: float, \n",
    "                                 cutoff_r: float, \n",
    "                                 max_num_neighbors: int = 100,\n",
    "                                 eps: float = 0.01,\n",
    "                                 cluster_method: str = 'knn'):\n",
    "    assert x.ndim == 2 and x.shape[-1] == 3, f\"{x.shape}\" # (nX, 3)\n",
    "    assert y.ndim == 3 and y.shape[-1] == 3, f\"{y.shape}\" # (nPose, nY, 3)\n",
    "    assert Ts.ndim == 2 and Ts.shape[-1] == 7, f\"{Ts.shape}\" # (nPose, 7)\n",
    "    assert len(Ts) == len(y), f\"{Ts.shape}, {y.shape}\"\n",
    "    n_poses, n_y_points = y.shape[:2]\n",
    "\n",
    "    Ty = transform_points(y, Ts, batched_pcd=True) # (nPose, nY, 3)\n",
    "    energy, grad = _pcd_energy(\n",
    "        x=x, \n",
    "        y=Ty, \n",
    "        cutoff_r=cutoff_r, \n",
    "        eps = eps, \n",
    "        max_num_neighbor=max_num_neighbors, \n",
    "        cluster_method=cluster_method\n",
    "    ) # (nPose,), (nPose, 6)\n",
    "    assert isinstance(grad, torch.Tensor)\n",
    "    grad = _se3_adjoint_lie_grad(Ts, grad) # (nPose, 6)\n",
    "\n",
    "    # disp = -grad / (grad.norm() + eps) * dt\n",
    "    grad = grad * (torch.tensor([1., 1., 1., cutoff_r, cutoff_r, cutoff_r], device=grad.device, dtype=grad.dtype))\n",
    "    disp = -grad * dt * cutoff_r\n",
    "    disp_pose = data.se3._exp_map(disp) # (n_poses, 7)\n",
    "\n",
    "    new_pose = data.se3._multiply(Ts, disp_pose)\n",
    "\n",
    "    # done = torch.isclose(energy, torch.zeros_like(energy))\n",
    "\n",
    "    return new_pose, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp_pose, y_new, energy = _optimize_pcd_collision_once(x=x, y=y, dt=0.0001, cutoff_r=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = [target_poses.poses]\n",
    "for i in range(30):\n",
    "    new_pose, energy = _optimize_pcd_collision_once(x=scene_pcd.points, y=grasp_pcd.points.unsqueeze(0), Ts=poses[-1], dt=0.00003, cutoff_r=0.03)\n",
    "    poses.append(new_pose)\n",
    "poses = torch.cat(poses, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.TargetPoseDemo(scene_pcd=scene_pcd,grasp_pcd=grasp_pcd, target_poses=data.SE3(poses=poses)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.show(width=600,height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.PointCloud.merge(scene_pcd, grasp_pcd.new(points=y_new[0])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy, grad = _pcd_energy(x,y,cutoff_r=0.05, eps = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_root_dir, 'agent.yaml')) as f:\n",
    "    model_kwargs_list = yaml.load(f, Loader=yaml.FullLoader)['model_kwargs'][f\"{task_type}_models_kwargs\"]\n",
    "\n",
    "with open(os.path.join(config_root_dir, 'preprocess.yaml')) as f:\n",
    "    preprocess_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    unprocess_config = preprocess_config['unprocess_config']\n",
    "    preprocess_config = preprocess_config['preprocess_config']\n",
    "\n",
    "agent = DiffusionEdfAgent(\n",
    "    model_kwargs_list=model_kwargs_list,\n",
    "    preprocess_config=preprocess_config,\n",
    "    unprocess_config=unprocess_config,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Input Data and Initial Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo: TargetPoseDemo = testset[0][0 if task_type == 'pick' else 1 if task_type == 'place' else \"task_type must be either 'pick' or 'place'\"].to(device)\n",
    "scene_pcd: PointCloud = demo.scene_pcd\n",
    "grasp_pcd: PointCloud = demo.grasp_pcd\n",
    "T0 = torch.cat([\n",
    "    torch.tensor([[1., 0., 0.0, 0.]], device=device),\n",
    "    torch.tensor([[0., 0., 0.8]], device=device)\n",
    "], dim=-1)\n",
    "Ts_init = SE3(poses=T0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts_out, scene_proc, grasp_proc = agent.sample(scene_pcd=scene_pcd, grasp_pcd=grasp_pcd, Ts_init=Ts_init,\n",
    "                                              N_steps_list = [[500, 500], [500, 1000]],\n",
    "                                              timesteps_list = [[0.02, 0.02], [0.02, 0.02]],\n",
    "                                              temperature_list = [1., 1.],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "visualization = TargetPoseDemo(\n",
    "    target_poses=SE3(poses=torch.cat([Ts_out[::10, sample_idx], Ts_out[-1:, sample_idx]], dim=0)),\n",
    "    scene_pcd=scene_proc,\n",
    "    grasp_pcd=grasp_proc\n",
    ")\n",
    "visualization = agent.unprocess_fn(visualization).to('cpu')\n",
    "visualization.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
